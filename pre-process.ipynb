{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "678c939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from getpass import getpass\n",
    "import re\n",
    "import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9620f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 27017\n",
    "\n",
    "user = getpass(\"user mongo\")\n",
    "password = getpass(\"password mongo\")\n",
    "database_name = getpass(\"database name\")\n",
    "\n",
    "uri = f\"mongodb://{user}:{password}@{host}:{port}?authSource=admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d83d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(uri)\n",
    "db = client[database_name]\n",
    "collection = db[\"sentiment-analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21bc955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889312\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"source\": \"portuguese-tweets-for-sentiment-analysis\"\n",
    "}\n",
    "\n",
    "print(collection.count_documents(query))\n",
    "\n",
    "response = collection.find(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25ab5887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\miniconda3\\lib\\site-packages\\spirecomm-0.6.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58bca36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.pt.Portuguese at 0x1afbb3034a0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from num2words import num2words\n",
    "from unidecode import unidecode\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "978a8b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'treze mil, quinhentos e cinquenta e seis vírgula três'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code genereted by chatgpt\n",
    "def numeros_por_extenso_pt(texto: str) -> str:\n",
    "    def _convert(match):\n",
    "        token = match.group(0)\n",
    "        # normaliza separadores decimais pt/en\n",
    "        if \",\" in token and \".\" in token:\n",
    "            # Ex.: 1.234,56 → remove pontos de milhar e trata vírgula como decimal\n",
    "            inteiro, frac = token.replace(\".\", \"\").split(\",\", 1)\n",
    "            inteiro_w = num2words(int(inteiro), lang=\"pt_BR\")\n",
    "            frac_w = \" \".join(num2words(int(d), lang=\"pt_BR\") for d in frac)\n",
    "            return f\"{inteiro_w} vírgula {frac_w}\"\n",
    "        elif \",\" in token:\n",
    "            inteiro, frac = token.split(\",\", 1)\n",
    "            inteiro_w = num2words(int(inteiro), lang=\"pt_BR\")\n",
    "            frac_w = \" \".join(num2words(int(d), lang=\"pt_BR\") for d in frac)\n",
    "            return f\"{inteiro_w} vírgula {frac_w}\"\n",
    "        elif \".\" in token:\n",
    "            # Pode ser decimal estilo EN ou milhar. Se parecer decimal curto, trata como decimal.\n",
    "            partes = token.split(\".\")\n",
    "            if len(partes) == 2 and len(partes[1]) <= 2:\n",
    "                inteiro, frac = partes\n",
    "                inteiro_w = num2words(int(inteiro), lang=\"pt_BR\")\n",
    "                frac_w = \" \".join(num2words(int(d), lang=\"pt_BR\") for d in frac)\n",
    "                return f\"{inteiro_w} ponto {frac_w}\"\n",
    "            # senão: milhar -> remove pontos e converte inteiro\n",
    "            token = token.replace(\".\", \"\")\n",
    "        # inteiro puro\n",
    "        try:\n",
    "            return num2words(int(token), lang=\"pt_BR\")\n",
    "        except ValueError:\n",
    "            return token  # fallback\n",
    "\n",
    "    # \\d{1,3}(\\.\\d{3})* para milhar com ponto | \\d+ para inteiros | vírgula/ponto para decimais\n",
    "    padrao = re.compile(r'\\b\\d{1,3}(?:\\.\\d{3})*(?:[,\\.]\\d+)?\\b')\n",
    "    return padrao.sub(_convert, texto)\n",
    "\n",
    "\n",
    "numeros_por_extenso_pt(\"13.556,3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1740c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "889312it [58:30, 253.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    doc = nlp(text)    \n",
    "    tokens_limpos = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    return \" \".join(tokens_limpos)\n",
    "\n",
    "\n",
    "for doc in tqdm.tqdm(response):\n",
    "    text = doc['text']\n",
    "    collection.update_one(\n",
    "        {\"_id\": doc[\"_id\"]},\n",
    "        {\"$set\": {\"preprocessed_text\": preprocess(text),\n",
    "                  \"text_processed_at\": datetime.datetime.now(datetime.timezone.utc)}}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
